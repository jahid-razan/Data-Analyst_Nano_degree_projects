{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem statement\n",
    "\n",
    "Data is fun to play with when it clean and meaningful insights can be harnessed from it to formulate strategy and make evidence based decision. Easier said than done because- as is the case with most other practical things in life, the world of data too is messy and data rarely comes as clean. \n",
    "\n",
    "Hence, the job of a data analyst involves a great deal of preprocessing of data which can consume up to 80% of the effort before even start getting meaningful insight from it [1]. Hence, it is very fundamental for a data analyst or data enthusiast to be able to master the skills of gathering, assessing, and cleaning data that are collected from a variety of sources and formats. Also, it is important for somebody who plays with data to be aware of the quality and tidiness issues. \n",
    "\n",
    "Twitter is one of the most popular social media platforms with more than 100 million users and more than 340 million tweets. It accounts for a 1.6 billion years queries per day. What makes twitter particularly effective is its precision as users are only allowed to use a limited number of characters [2]. Users use Twitter to share interesting items that includes their hobbies, profession, research articles and updates and latest happenings. Hence it can be used as a very effective data source. Also, twitter data can be accesses using API which has been done in this case. \n",
    "\n",
    "\n",
    "The dataset in this wrangling project has been obtained from the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators are almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because they're good dogs. WeRateDogs has over 4 million followers and has received international media coverage.\n",
    "\n",
    "\n",
    "The objective of this project is to use Python and its libraries, to gather data from a variety of sources and in a variety of formats, assess its quality and tidiness, then clean it to create interesting and trustworthy analyses and visualizations. Also, at the end of the analysis, the master data set will be saved in the csv format and some insights have been discussed from. The crucial steps of the whole journey will be explained in a way so that the actions can be followed and the results can be reproduced.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Sources and Notebook Files\n",
    "\n",
    "\n",
    "1. The WeRateDogs Twitter archive: twitter_archive_enhanced.csv has been received from the Udacity instructor and downloded.\n",
    "\n",
    "\n",
    "2. A file (image_predictions.tsv) is hosted on Udacity's servers and  has been downloaded programmatically using the Requests library and the specified following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv.\n",
    "\n",
    "\n",
    "3. Using the tweet IDs in the WeRateDogs Twitter archive, query have been made of the Twitter API for each tweet's JSON data using Python's Tweepy library and entire set of JSON data has been stored in a file called tweet_json.txt file. \n",
    "\n",
    "\n",
    "4. separate Jupyter notebooks has been used during the project. \n",
    "\n",
    "\n",
    "**Wrangle_act_1**: Data have been gather using the sources mentioned in 1,2 and 3\n",
    "\n",
    "**Wrangle_act_2**: Deals with data quality and tidyness issues of the twitter_archive_enhanced.csv file. \n",
    "\n",
    "**Wrangle_act_3**: Deals with the data quality and tidyness issues of the image_predictions.tsv file\n",
    "\n",
    "**Wrangle_act_4**: In this file the data quality and tidyness issues of the twitter data obtained using the API has been dealt. Then all the dataframe has been merged, and the vizualizations have been made. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Platform and Software Packages\n",
    "\n",
    "The project has been done using Jupyater Notebook and the following packages has been used for the project: \n",
    "    \n",
    "- pandas\n",
    "\n",
    "- NumPy\n",
    "\n",
    "- requests\n",
    "\n",
    "- tweepy\n",
    "\n",
    "- json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Assessing Data for this Project and cleaning.\n",
    "\n",
    "After gathering each of the above pieces of data, a visual and and programmatic assessment has been donr for quality and tidiness issues. The project has listed Nine (9) quality issues and two (2) tidiness issues in the Jupyter Notebooks to meet specifications.The issues have been defined, coded for correction and tested.\n",
    "\n",
    "\n",
    "**4.A Data Quality issues in the df_twitter dataframe obtained from the csv file.**\n",
    "\n",
    "1.Get rid of the rows with value in the retweeted_status.\n",
    "\n",
    "2.Define Quality Issue 2. Drop columns that we do not need:\n",
    "\n",
    "   I. in_reply_to_status_id\n",
    "\n",
    "   II. in_reply_to_user_id\n",
    "\n",
    "   III. retweeted_status_id\n",
    "\n",
    "   IV. retweeted_status_user_id\n",
    "\n",
    "   V. retweeted_status_timestamp\n",
    "   \n",
    "   VI. Source\n",
    "   \n",
    "  VII. text\n",
    "   \n",
    "   VIII. extended_urls\n",
    "   \n",
    "   \n",
    "3. Drop the values that do not have 10 in the denominator.\n",
    "\n",
    "\n",
    "4. Drop the values that have numerator over 20.\n",
    "\n",
    "\n",
    "5. Timestamp is a string. Convert Timestamp column into datetime format.\n",
    "\n",
    "\n",
    "**4.B Data Issues Quality issues in the df_image_pred dataframe obtained from the url.**\n",
    "\n",
    "6. Get rid of the rows that do not add value such as:  jpg_url, img_num.\n",
    "\n",
    "\n",
    "7. Capitalize all the first letters of p1, p2 and p3 column.\n",
    "\n",
    "\n",
    "8. Remove the symbol '_' in between the p1 ,p2 and p3 string values.\n",
    "\n",
    "**4.C Data Quality issues in the df_image_pred dataframe obtained from the url.**\n",
    "\n",
    "9. Rename 'id' to 'tweet_id' in the df_tweet_info dataframe usin the rename function . \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**4.D Tidyness Issues in the df_twitter dataframe obtained from the csv file.**\n",
    "\n",
    "1. The datetime format is difficult to read. Extract Year, Month, and Day and hour from it.\n",
    "\n",
    "2. Bring every dog stage information to rows and get rid of the stage columns.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Overview\n",
    "\n",
    "After cleaning the dataset all the three dataframes have been merged into a df_master file.\n",
    "\n",
    "\n",
    "### A. What is the structure of this dataset?\n",
    "\n",
    "The df_master data consists of of 1958 rows and  20 columns. There are 3 boolean variables, 3 floats, 7 integar, and 7 strings. The variables provide information about the name of the dog, time of tweet-day, month, hour, and year, dog stages, p1, p2, p3- the algorithm's #1, #2, #3 prediction for the image in the tweet, p1_conf, p2_conf, p3_conf- the algorithm's first, second and third most likely prediction, favorite_count and retweet_count. \n",
    "\n",
    "\n",
    "### B. What is/are the main feature(s) of interest in the dataset?\n",
    "\n",
    "I'm most interested in figuring out:\n",
    "\n",
    "1. Most frequent dog genres predicted by the algorithm\n",
    "\n",
    "\n",
    "2. Most freuqunet dog stages\n",
    "\n",
    "\n",
    "3. Time- Year, Month and hour of tweeting\n",
    "\n",
    "\n",
    "4. Popular dog names\n",
    "\n",
    "\n",
    "5. Correlation coeffiecnt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Observations and Conclusion\n",
    "\n",
    "During the vizualization process certain observations have been made and registered after every vizualization. At the end of the analysis the cleaned master dataset has been saved in csv format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ref\n",
    "\n",
    "1.  Tidy data by Hadley Wickham, The Journal of Statistical Software, vol. 59, 2014.\n",
    "\n",
    "2. Wikipedia page on twitter, accesseed on the 29 Jan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
